{"cells":[{"cell_type":"markdown","metadata":{"id":"uL9gmGg_7O6V"},"source":["# ğŸ¯ Hyperparameter Tuning Round 3 - Optimized Fine-Tuning\n","### Player Direction Prediction - Data-Driven Parameter Selection\n","\n","**Round 2 Best:** MAE=51.39Â° (LR=1.5e-04, WD=5e-04, DR=0.55)\n","\n","**Round 3 Strategy** (Based on statistical analysis):\n","- âœ… Fixed BS=32 (proven optimal)\n","- âœ… Fixed WD=1e-04 (most consistent, CV=0.045)\n","- ğŸ” Explore LR around 1.8e-04 (best mean+std score)\n","- ğŸ” Explore DR around 0.55 (best minimum)\n","- **Total: 30 experiments (~3 hours on A100)**"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"08nrx01yaqLk","executionInfo":{"status":"ok","timestamp":1766531393820,"user_tz":-180,"elapsed":2110,"user":{"displayName":"ofcosar","userId":"12337476720676369762"}},"outputId":"edd28680-dd35-4cb5-98ff-973e24e00c67"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"markdown","metadata":{"id":"YAJtu9KL7O6Y"},"source":["## 1. Setup & Imports"]},{"cell_type":"code","execution_count":33,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KfWDOLrk7O6Y","outputId":"6243bfca-3b3f-4571-ee3a-776a70b08f68","executionInfo":{"status":"ok","timestamp":1766531393838,"user_tz":-180,"elapsed":16,"user":{"displayName":"ofcosar","userId":"12337476720676369762"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… Round 3: Optimized Fine-Tuning\n","   Total: 30 experiments\n","   LR: ['1.26e-04', '1.53e-04', '1.80e-04', '2.07e-04', '2.34e-04', '2.70e-04']\n","   WD (FIXED): 1e-04 â­\n","   DR: [0.45, 0.5, 0.55, 0.6, 0.65]\n","   Time: ~3 hours on A100\n"]}],"source":["# Base configuration\n","BASE_CONFIG = {\n","    'data_root': '/content/drive/MyDrive/player_direction_dataset',\n","    'results_dir': '/content/drive/MyDrive/hyperparameter_tuning_results_round3',\n","    'num_workers': 2,\n","    'pin_memory': True,\n","}\n","\n","# ROUND 3: OPTIMIZED (30 experiments)\n","# Proof: WD=1e-04 has lowest CV (0.045), LR=1.8e-04 best mean+std\n","HYPERPARAMETER_GRID = {\n","    'learning_rate': [\n","        1.26e-04,  # -30%\n","        1.53e-04,  # -15%\n","        1.80e-04,  # Center (best from Round 2)\n","        2.07e-04,  # +15%\n","        2.34e-04,  # +30%\n","        2.70e-04   # +50%\n","    ],\n","    'batch_size': [32],\n","    'weight_decay': [1e-04],  # FIXED (most consistent)\n","    'dropout': [0.45, 0.50, 0.55, 0.60, 0.65],\n","}\n","\n","FIXED_PARAMS = {\n","    'backbone': 'resnet34',\n","    'num_epochs': 100,\n","    'early_stopping_patience': 20,\n","    'early_stopping_min_delta': 0.5,\n","    'gradient_accumulation_steps': 2,\n","    'use_amp': True,\n","}\n","\n","total_combinations = (len(HYPERPARAMETER_GRID['learning_rate']) *\n","                     len(HYPERPARAMETER_GRID['batch_size']) *\n","                     len(HYPERPARAMETER_GRID['weight_decay']) *\n","                     len(HYPERPARAMETER_GRID['dropout']))\n","\n","assert total_combinations == 30\n","assert len(set(HYPERPARAMETER_GRID['learning_rate'])) == 6\n","\n","print(f\"âœ… Round 3: Optimized Fine-Tuning\")\n","print(f\"   Total: {total_combinations} experiments\")\n","print(f\"   LR: {[f'{x:.2e}' for x in HYPERPARAMETER_GRID['learning_rate']]}\")\n","print(f\"   WD (FIXED): {HYPERPARAMETER_GRID['weight_decay'][0]:.0e} â­\")\n","print(f\"   DR: {HYPERPARAMETER_GRID['dropout']}\")\n","print(f\"   Time: ~3 hours on A100\")"]},{"cell_type":"code","execution_count":34,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pDYcKd8l7O6a","outputId":"75053aa8-ee97-4f23-8025-b020996beded","executionInfo":{"status":"ok","timestamp":1766531393844,"user_tz":-180,"elapsed":5,"user":{"displayName":"ofcosar","userId":"12337476720676369762"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… Using device: cuda\n","   GPU: Tesla T4\n","   Memory: 15.8 GB\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","from torch.cuda.amp import autocast, GradScaler\n","from torchvision import transforms, models\n","from pathlib import Path\n","import json\n","import math\n","import numpy as np\n","from PIL import Image\n","from tqdm import tqdm\n","import time\n","import pandas as pd\n","from datetime import datetime\n","import itertools\n","import os\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f\"âœ… Using device: {device}\")\n","\n","if torch.cuda.is_available():\n","    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n","    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"]},{"cell_type":"markdown","metadata":{"id":"XQ36eTFO7O6a"},"source":["## 2. Configuration"]},{"cell_type":"markdown","metadata":{"id":"IT3D0jLq7O6b"},"source":["## 3. Dataset Class"]},{"cell_type":"code","execution_count":35,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YWYFs6b-7O6b","outputId":"3db9b5ca-f045-4b0b-be12-53bf933e52f2","executionInfo":{"status":"ok","timestamp":1766531393862,"user_tz":-180,"elapsed":17,"user":{"displayName":"ofcosar","userId":"12337476720676369762"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… Dataset class defined\n"]}],"source":["class PlayerDirectionDataset(Dataset):\n","    def __init__(self, root_dir, split='train', transform=None):\n","        if split == 'val':\n","            split = 'valid'\n","\n","        self.root_dir = Path(root_dir) / split\n","        self.transform = transform\n","\n","        labels_file = self.root_dir / 'labels.json'\n","        with open(labels_file, 'r') as f:\n","            labels_list = json.load(f)\n","\n","        self.labels = {item['filename']: item['direction_degree'] for item in labels_list}\n","        self.image_files = list(self.labels.keys())\n","\n","    def __len__(self):\n","        return len(self.image_files)\n","\n","    def __getitem__(self, idx):\n","        img_name = self.image_files[idx]\n","        img_path = self.root_dir / 'images' / img_name\n","\n","        image = Image.open(img_path).convert('RGB')\n","        angle_deg = self.labels[img_name]\n","\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        angle_rad = math.radians(angle_deg)\n","\n","        return {\n","            'image': image,\n","            'sin': torch.tensor(math.sin(angle_rad), dtype=torch.float32),\n","            'cos': torch.tensor(math.cos(angle_rad), dtype=torch.float32),\n","            'angle_deg': torch.tensor(angle_deg, dtype=torch.float32)\n","        }\n","\n","print(\"âœ… Dataset class defined\")"]},{"cell_type":"markdown","metadata":{"id":"LTJc-xhP7O6c"},"source":["## 4. Model Architecture"]},{"cell_type":"code","execution_count":36,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fPd2yf4d7O6c","outputId":"df877076-11e2-41e5-9342-a12887d2b704","executionInfo":{"status":"ok","timestamp":1766531393868,"user_tz":-180,"elapsed":4,"user":{"displayName":"ofcosar","userId":"12337476720676369762"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… Model architecture defined\n"]}],"source":["class PlayerDirectionPredictor(nn.Module):\n","    def __init__(self, dropout=0.5):\n","        super().__init__()\n","\n","        resnet = models.resnet34(weights=models.ResNet34_Weights.IMAGENET1K_V1)\n","        self.features = nn.Sequential(*list(resnet.children())[:-1])\n","\n","        self.head = nn.Sequential(\n","            nn.Flatten(),\n","            nn.Linear(512, 256),\n","            nn.ReLU(),\n","            nn.Dropout(dropout),\n","            nn.Linear(256, 128),\n","            nn.ReLU(),\n","            nn.Dropout(dropout),\n","            nn.Linear(128, 2)\n","        )\n","\n","    def forward(self, x):\n","        features = self.features(x)\n","        sin_cos = self.head(features)\n","        sin_cos = nn.functional.normalize(sin_cos, p=2, dim=1)\n","        return sin_cos\n","\n","    def predict_angle(self, x):\n","        sin_cos = self.forward(x)\n","        angle_rad = torch.atan2(sin_cos[:, 0], sin_cos[:, 1])\n","        angle_deg = torch.rad2deg(angle_rad) % 360\n","        return angle_deg\n","\n","class CircularLoss(nn.Module):\n","    def forward(self, pred, target_sin, target_cos):\n","        pred_sin, pred_cos = pred[:, 0], pred[:, 1]\n","        loss = 1 - (pred_sin * target_sin + pred_cos * target_cos)\n","        return loss.mean()\n","\n","print(\"âœ… Model architecture defined\")"]},{"cell_type":"markdown","metadata":{"id":"vZG7enQi7O6c"},"source":["## 5. Training Functions with Mixed Precision"]},{"cell_type":"code","execution_count":37,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4z6nMNmL7O6c","outputId":"74f52ade-2ac6-460d-babd-453ffecd63a5","executionInfo":{"status":"ok","timestamp":1766531393887,"user_tz":-180,"elapsed":17,"user":{"displayName":"ofcosar","userId":"12337476720676369762"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… Training functions defined with mixed precision\n"]}],"source":["def calculate_accuracy_metrics(pred_angles, target_angles, thresholds=[15, 30, 45]):\n","    \"\"\"Calculate accuracy at different angle thresholds\"\"\"\n","    errors = torch.abs(pred_angles - target_angles)\n","    # Handle circular nature (e.g., 359Â° and 1Â° are close)\n","    errors = torch.min(errors, 360 - errors)\n","\n","    metrics = {}\n","    for threshold in thresholds:\n","        acc = (errors <= threshold).float().mean().item() * 100\n","        metrics[f'acc{threshold}'] = acc\n","\n","    return metrics\n","\n","def train_epoch(model, loader, criterion, optimizer, device, use_amp=True, accumulation_steps=1):\n","    \"\"\"Train one epoch with mixed precision and gradient accumulation\"\"\"\n","    model.train()\n","    total_loss = 0\n","    all_pred_angles = []\n","    all_target_angles = []\n","\n","    scaler = GradScaler(enabled=use_amp)\n","    optimizer.zero_grad()\n","\n","    for batch_idx, batch in enumerate(loader):\n","        images = batch['image'].to(device)\n","        target_sin = batch['sin'].to(device)\n","        target_cos = batch['cos'].to(device)\n","        target_angles = batch['angle_deg'].to(device)\n","\n","        # Mixed precision forward pass\n","        with autocast(enabled=use_amp):\n","            pred = model(images)\n","            loss = criterion(pred, target_sin, target_cos)\n","            # Scale loss for gradient accumulation\n","            loss = loss / accumulation_steps\n","\n","        # Backward pass with gradient scaling\n","        scaler.scale(loss).backward()\n","\n","        # Update weights every accumulation_steps\n","        if (batch_idx + 1) % accumulation_steps == 0 or (batch_idx + 1) == len(loader):\n","            scaler.unscale_(optimizer)\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","            scaler.step(optimizer)\n","            scaler.update()\n","            optimizer.zero_grad()\n","\n","        # Calculate metrics\n","        with torch.no_grad():\n","            pred_angles = model.predict_angle(images)\n","            all_pred_angles.append(pred_angles)\n","            all_target_angles.append(target_angles)\n","\n","        total_loss += loss.item() * accumulation_steps\n","\n","    all_pred_angles = torch.cat(all_pred_angles)\n","    all_target_angles = torch.cat(all_target_angles)\n","\n","    mae = torch.mean(torch.abs(all_pred_angles - all_target_angles)).item()\n","    metrics = calculate_accuracy_metrics(all_pred_angles, all_target_angles)\n","\n","    return total_loss / len(loader), mae, metrics\n","\n","def validate(model, loader, criterion, device, use_amp=True):\n","    \"\"\"Validate with mixed precision\"\"\"\n","    model.eval()\n","    total_loss = 0\n","    all_pred_angles = []\n","    all_target_angles = []\n","\n","    with torch.no_grad():\n","        for batch in loader:\n","            images = batch['image'].to(device)\n","            target_sin = batch['sin'].to(device)\n","            target_cos = batch['cos'].to(device)\n","            target_angles = batch['angle_deg'].to(device)\n","\n","            with autocast(enabled=use_amp):\n","                pred = model(images)\n","                loss = criterion(pred, target_sin, target_cos)\n","\n","            pred_angles = model.predict_angle(images)\n","            all_pred_angles.append(pred_angles)\n","            all_target_angles.append(target_angles)\n","\n","            total_loss += loss.item()\n","\n","    all_pred_angles = torch.cat(all_pred_angles)\n","    all_target_angles = torch.cat(all_target_angles)\n","\n","    mae = torch.mean(torch.abs(all_pred_angles - all_target_angles)).item()\n","    metrics = calculate_accuracy_metrics(all_pred_angles, all_target_angles)\n","\n","    return total_loss / len(loader), mae, metrics\n","\n","print(\"âœ… Training functions defined with mixed precision\")"]},{"cell_type":"markdown","metadata":{"id":"a4-PRNiQ7O6d"},"source":["## 6. Experiment Management"]},{"cell_type":"code","execution_count":38,"metadata":{"id":"G2Vnp2J67O6d","executionInfo":{"status":"ok","timestamp":1766531393909,"user_tz":-180,"elapsed":11,"user":{"displayName":"ofcosar","userId":"12337476720676369762"}}},"outputs":[],"source":["def get_experiment_name(params):\n","    \"\"\"Generate unique experiment name from parameters\"\"\"\n","    # Use 2 decimals for dropout to avoid rounding (0.55 != 0.6)\n","    # Changed LR and WD to .2e for better precision and unique names\n","    return f\"lr{params['learning_rate']:.2e}_bs{params['batch_size']}_wd{params['weight_decay']:.2e}_dr{params['dropout']:.2f}\""]},{"cell_type":"markdown","metadata":{"id":"validation_header"},"source":["## 6.5. ğŸ” Validate Grid (Check for Duplicates)"]},{"cell_type":"code","execution_count":39,"metadata":{"id":"validation_code","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1766531393926,"user_tz":-180,"elapsed":16,"user":{"displayName":"ofcosar","userId":"12337476720676369762"}},"outputId":"17bc1501-6c66-4806-be5b-e743773bb68c"},"outputs":[{"output_type":"stream","name":"stdout","text":["ğŸ” Validating hyperparameter grid...\n","======================================================================\n","âœ… Total combinations: 30\n","âœ… LR values: [0.000126, 0.000153, 0.00018, 0.000207, 0.000234, 0.00027]\n","\n","âœ… SUCCESS: All 30 experiment names are UNIQUE!\n","\n","   Sample: lr1.26e-04_bs32_wd1.00e-04_dr0.45, lr1.26e-04_bs32_wd1.00e-04_dr0.50, ...\n","\n","======================================================================\n","âœ… Validation PASSED!\n","======================================================================\n"]}],"source":["# Validate hyperparameter grid\n","from collections import Counter\n","\n","print(\"ğŸ” Validating hyperparameter grid...\")\n","print(\"=\" * 70)\n","\n","param_combinations_test = list(itertools.product(\n","    HYPERPARAMETER_GRID['learning_rate'],\n","    HYPERPARAMETER_GRID['batch_size'],\n","    HYPERPARAMETER_GRID['weight_decay'],\n","    HYPERPARAMETER_GRID['dropout']\n","))\n","\n","print(f\"âœ… Total combinations: {len(param_combinations_test)}\")\n","print(f\"âœ… LR values: {sorted(set([x[0] for x in param_combinations_test]))}\")\n","\n","exp_names = []\n","for lr, bs, wd, dropout in param_combinations_test:\n","    params = {'learning_rate': lr, 'batch_size': bs, 'weight_decay': wd, 'dropout': dropout}\n","    exp_name = get_experiment_name(params)\n","    exp_names.append(exp_name)\n","\n","name_counts = Counter(exp_names)\n","duplicates = {name: count for name, count in name_counts.items() if count > 1}\n","\n","if duplicates:\n","    print(f\"\\nâŒ ERROR: Found {len(duplicates)} duplicate names!\")\n","    for i, (name, count) in enumerate(list(duplicates.items())[:10], 1):\n","        print(f\"   {i}. {name}: {count} times\")\n","    raise ValueError(f\"Fix HYPERPARAMETER_GRID in Cell 2!\")\n","else:\n","    print(f\"\\nâœ… SUCCESS: All {len(exp_names)} experiment names are UNIQUE!\")\n","    print(f\"\\n   Sample: {exp_names[0]}, {exp_names[1]}, ...\")\n","\n","print(\"\\n\" + \"=\" * 70)\n","print(\"âœ… Validation PASSED!\")\n","print(\"=\" * 70)"]},{"cell_type":"markdown","metadata":{"id":"cleanup_header"},"source":["## 6.6. ğŸ§¹ Cleanup Duplicates (Run Once Before Training)"]},{"cell_type":"code","execution_count":40,"metadata":{"id":"cleanup_code","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1766531393936,"user_tz":-180,"elapsed":10,"user":{"displayName":"ofcosar","userId":"12337476720676369762"}},"outputId":"603ed270-d9ed-414f-b7d4-1e1e8a358e80"},"outputs":[{"output_type":"stream","name":"stdout","text":["ğŸ§¹ CLEANING UP DUPLICATES\n","======================================================================\n","\n","ğŸ“Š Step 1: Cleaning Excel...\n","   âš ï¸  Excel not found\n","\n","ğŸ“ Step 2: Cleaning folders...\n","\n","======================================================================\n","âœ… CLEANUP COMPLETE!\n","======================================================================\n"]}],"source":["# Clean up duplicate experiments from folder and Excel\n","import shutil\n","from collections import defaultdict\n","\n","print(\"ğŸ§¹ CLEANING UP DUPLICATES\")\n","print(\"=\" * 70)\n","\n","results_dir = Path(BASE_CONFIG['results_dir'])\n","excel_file = results_dir / 'hyperparameter_tuning_results.xlsx'\n","\n","# Step 1: Clean Excel\n","print(\"\\nğŸ“Š Step 1: Cleaning Excel...\")\n","if excel_file.exists():\n","    df = pd.read_excel(excel_file)\n","    print(f\"   Original: {len(df)} experiments\")\n","\n","    df['config'] = df.apply(\n","        lambda x: f\"{x['learning_rate']:.0e}_{x['batch_size']}_{x['weight_decay']:.0e}_{x['dropout']:.2f}\",\n","        axis=1\n","    )\n","\n","    duplicates = df[df.duplicated(subset='config', keep='first')]\n","\n","    if len(duplicates) > 0:\n","        print(f\"   Found {len(duplicates)} duplicates, removing...\")\n","        shutil.copy(excel_file, results_dir / 'results_BACKUP.xlsx')\n","        df_clean = df.drop_duplicates(subset='config', keep='first').drop(columns=['config'])\n","        df_clean.to_excel(excel_file, index=False, engine='openpyxl')\n","        print(f\"   âœ… Cleaned: {len(df_clean)} unique experiments\")\n","    else:\n","        print(f\"   âœ… No duplicates in Excel\")\n","else:\n","    print(f\"   âš ï¸  Excel not found\")\n","\n","# Step 2: Clean folders\n","print(\"\\nğŸ“ Step 2: Cleaning folders...\")\n","if results_dir.exists():\n","    folders = [d for d in results_dir.iterdir() if d.is_dir()]\n","    print(f\"   Found {len(folders)} folders\")\n","\n","    experiments_by_config = defaultdict(list)\n","\n","    for folder in folders:\n","        result_file = folder / 'result.json'\n","        if result_file.exists():\n","            with open(result_file, 'r') as f:\n","                result = json.load(f)\n","                p = result['params']\n","                key = f\"{p['learning_rate']:.0e}_{p['batch_size']}_{p['weight_decay']:.0e}_{p['dropout']:.2f}\"\n","                experiments_by_config[key].append({\n","                    'folder': folder,\n","                    'mae': result['best_val_mae']\n","                })\n","\n","    to_remove = []\n","    for key, exps in experiments_by_config.items():\n","        if len(exps) > 1:\n","            print(f\"   Config {key}: {len(exps)} folders\")\n","            best = min(exps, key=lambda x: x['mae'])\n","            for exp in exps:\n","                if exp != best:\n","                    print(f\"      Remove: {exp['folder'].name} (MAE={exp['mae']:.2f}Â°)\")\n","                    to_remove.append(exp['folder'])\n","                else:\n","                    print(f\"      Keep:   {exp['folder'].name} (MAE={exp['mae']:.2f}Â°) â­\")\n","\n","    if to_remove:\n","        print(f\"\\n   Removing {len(to_remove)} duplicate folders...\")\n","        for folder in to_remove:\n","            shutil.rmtree(folder)\n","        print(f\"   âœ… Removed {len(to_remove)} folders\")\n","    else:\n","        print(f\"   âœ… No duplicate folders\")\n","\n","print(\"\\n\" + \"=\" * 70)\n","print(\"âœ… CLEANUP COMPLETE!\")\n","print(\"=\" * 70)"]},{"cell_type":"markdown","metadata":{"id":"uCv7IWsS7O6d"},"source":["## 7. Main Training Loop"]},{"cell_type":"code","execution_count":41,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BxP8YvTS7O6e","outputId":"024dfbc9-ab00-4535-b36b-c1929153f88d","executionInfo":{"status":"ok","timestamp":1766531393942,"user_tz":-180,"elapsed":4,"user":{"displayName":"ofcosar","userId":"12337476720676369762"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… Main training loop defined\n"]}],"source":["def train_single_experiment(params, train_loader, val_loader, device, results_dir):\n","    \"\"\"Train a single hyperparameter configuration\"\"\"\n","\n","    exp_name = get_experiment_name(params)\n","    exp_dir = Path(results_dir) / exp_name\n","    exp_dir.mkdir(parents=True, exist_ok=True)\n","\n","    # Initialize model\n","    model = PlayerDirectionPredictor(dropout=params['dropout']).to(device)\n","    criterion = CircularLoss()\n","    optimizer = optim.AdamW(\n","        model.parameters(),\n","        lr=params['learning_rate'],\n","        weight_decay=params['weight_decay']\n","    )\n","    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n","        optimizer, mode='min', factor=0.5, patience=5, min_lr=1e-7\n","    )\n","\n","    best_val_mae = float('inf')\n","    best_val_acc15 = 0\n","    best_val_acc30 = 0\n","    best_val_acc45 = 0\n","    patience_counter = 0\n","    history = []\n","\n","    start_time = time.time()\n","\n","    for epoch in range(FIXED_PARAMS['num_epochs']):\n","        # Train\n","        train_loss, train_mae, train_metrics = train_epoch(\n","            model, train_loader, criterion, optimizer, device,\n","            use_amp=FIXED_PARAMS['use_amp'],\n","            accumulation_steps=FIXED_PARAMS['gradient_accumulation_steps']\n","        )\n","\n","        # Validate\n","        val_loss, val_mae, val_metrics = validate(\n","            model, val_loader, criterion, device,\n","            use_amp=FIXED_PARAMS['use_amp']\n","        )\n","\n","        # Update scheduler\n","        scheduler.step(val_mae)\n","\n","        # Save history\n","        epoch_data = {\n","            'epoch': epoch + 1,\n","            'train_loss': train_loss,\n","            'train_mae': train_mae,\n","            'train_acc15': train_metrics['acc15'],\n","            'train_acc30': train_metrics['acc30'],\n","            'train_acc45': train_metrics['acc45'],\n","            'val_loss': val_loss,\n","            'val_mae': val_mae,\n","            'val_acc15': val_metrics['acc15'],\n","            'val_acc30': val_metrics['acc30'],\n","            'val_acc45': val_metrics['acc45'],\n","            'lr': optimizer.param_groups[0]['lr']\n","        }\n","        history.append(epoch_data)\n","\n","        # Track best metrics\n","        if val_metrics['acc15'] > best_val_acc15:\n","            best_val_acc15 = val_metrics['acc15']\n","        if val_metrics['acc30'] > best_val_acc30:\n","            best_val_acc30 = val_metrics['acc30']\n","        if val_metrics['acc45'] > best_val_acc45:\n","            best_val_acc45 = val_metrics['acc45']\n","\n","        # Check for improvement\n","        if val_mae < best_val_mae - FIXED_PARAMS['early_stopping_min_delta']:\n","            best_val_mae = val_mae\n","            patience_counter = 0\n","\n","            # Save best model\n","            torch.save({\n","                'epoch': epoch + 1,\n","                'model_state_dict': model.state_dict(),\n","                'val_mae': val_mae,\n","                'val_metrics': val_metrics,\n","                'params': params\n","            }, exp_dir / 'best_model.pth')\n","        else:\n","            patience_counter += 1\n","\n","        # Early stopping\n","        if patience_counter >= FIXED_PARAMS['early_stopping_patience']:\n","            break\n","\n","    training_time = (time.time() - start_time) / 3600\n","\n","    # Save history\n","    with open(exp_dir / 'history.json', 'w') as f:\n","        json.dump(history, f, indent=2)\n","\n","    # Save final result\n","    result = {\n","        'experiment_name': exp_name,\n","        'params': params,\n","        'best_val_mae': best_val_mae,\n","        'best_val_acc15': best_val_acc15,\n","        'best_val_acc30': best_val_acc30,\n","        'best_val_acc45': best_val_acc45,\n","        'total_epochs': len(history),\n","        'training_time_hours': training_time,\n","        'completed': True,\n","        'timestamp': datetime.now().isoformat()\n","    }\n","\n","    with open(exp_dir / 'result.json', 'w') as f:\n","        json.dump(result, f, indent=2)\n","\n","    return result\n","\n","print(\"âœ… Main training loop defined\")"]},{"cell_type":"markdown","metadata":{"id":"do1yQ4rx7O6e"},"source":["## 8. Test Maximum Batch Size (Optional)"]},{"cell_type":"code","execution_count":42,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y06DHSIZ7O6e","outputId":"cc6dd820-fcdf-447a-8337-52d8ccdcf994","executionInfo":{"status":"ok","timestamp":1766531404512,"user_tz":-180,"elapsed":10567,"user":{"displayName":"ofcosar","userId":"12337476720676369762"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["ğŸ” Testing maximum batch size for your GPU...\n","\n","âœ… Batch size  32: OK (GPU: 4.7 GB)\n","âœ… Batch size  64: OK (GPU: 4.7 GB)\n","âœ… Batch size  96: OK (GPU: 4.7 GB)\n","âœ… Batch size 128: OK (GPU: 4.7 GB)\n","âœ… Batch size 192: OK (GPU: 4.7 GB)\n","âœ… Batch size 256: OK (GPU: 4.7 GB)\n","âœ… Batch size 320: OK (GPU: 4.7 GB)\n","âœ… Batch size 384: OK (GPU: 4.7 GB)\n","âœ… Batch size 448: OK (GPU: 4.7 GB)\n","âœ… Batch size 512: OK (GPU: 5.0 GB)\n","\n","ğŸ¯ Maximum batch size: 512\n","ğŸ’¡ Your hyperparameter grid includes: [32]\n","   All batch sizes should work fine!\n"]}],"source":["print(\"ğŸ” Testing maximum batch size for your GPU...\\n\")\n","\n","test_sizes = [32, 64, 96, 128, 192, 256, 320, 384, 448, 512]\n","max_batch_size = 32\n","\n","for bs in test_sizes:\n","    try:\n","        model = PlayerDirectionPredictor(dropout=0.5).to(device)\n","        test_batch = torch.randn(bs, 3, 224, 224).to(device)\n","\n","        with torch.no_grad():\n","            _ = model(test_batch)\n","\n","        max_batch_size = bs\n","        gpu_memory = torch.cuda.max_memory_allocated() / 1e9\n","        print(f\"âœ… Batch size {bs:3d}: OK (GPU: {gpu_memory:.1f} GB)\")\n","\n","        del model, test_batch\n","        torch.cuda.empty_cache()\n","\n","    except RuntimeError as e:\n","        if \"out of memory\" in str(e):\n","            print(f\"âŒ Batch size {bs:3d}: Out of memory\")\n","            break\n","        raise e\n","\n","print(f\"\\nğŸ¯ Maximum batch size: {max_batch_size}\")\n","print(f\"ğŸ’¡ Your hyperparameter grid includes: {HYPERPARAMETER_GRID['batch_size']}\")\n","print(f\"   All batch sizes should work fine!\")"]},{"cell_type":"markdown","metadata":{"id":"i6Qgi9hp7O6e"},"source":["## 9. Load Data (Once)"]},{"cell_type":"code","execution_count":43,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eNjs5D_m7O6e","outputId":"c45f97e8-dfec-45f9-a932-cb73a506f962","executionInfo":{"status":"ok","timestamp":1766531404534,"user_tz":-180,"elapsed":14,"user":{"displayName":"ofcosar","userId":"12337476720676369762"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","======================================================================\n","ğŸ“‚ LOADING DATASETS\n","======================================================================\n","\n","âœ… Train: 1400 images\n","âœ… Val: 300 images\n","======================================================================\n"]}],"source":["print(\"\\n\" + \"=\"*70)\n","print(\"ğŸ“‚ LOADING DATASETS\")\n","print(\"=\"*70)\n","\n","# Data transforms\n","train_transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.RandomHorizontalFlip(p=0.5),\n","    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","\n","val_transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","\n","# Load datasets\n","train_dataset = PlayerDirectionDataset(BASE_CONFIG['data_root'], 'train', train_transform)\n","val_dataset = PlayerDirectionDataset(BASE_CONFIG['data_root'], 'val', val_transform)\n","\n","print(f\"\\nâœ… Train: {len(train_dataset)} images\")\n","print(f\"âœ… Val: {len(val_dataset)} images\")\n","print(\"=\"*70)"]},{"cell_type":"markdown","metadata":{"id":"vc3GlJ6X7O6e"},"source":["## 10. Run Hyperparameter Tuning"]},{"cell_type":"code","execution_count":44,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gz2nmIgW7O6f","outputId":"0413ff24-f4b6-4500-a075-3e47ac9890f9","executionInfo":{"status":"ok","timestamp":1766553997504,"user_tz":-180,"elapsed":22592963,"user":{"displayName":"ofcosar","userId":"12337476720676369762"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["ğŸ” Checking for duplicate experiment names...\n","âœ… All 30 experiment names are unique!\n","\n","ğŸ“Š First 5 experiment names:\n","   1. lr1.26e-04_bs32_wd1.00e-04_dr0.45\n","   2. lr1.26e-04_bs32_wd1.00e-04_dr0.50\n","   3. lr1.26e-04_bs32_wd1.00e-04_dr0.55\n","   4. lr1.26e-04_bs32_wd1.00e-04_dr0.60\n","   5. lr1.26e-04_bs32_wd1.00e-04_dr0.65\n","\n","ğŸ“Š Last 5 experiment names:\n","   26. lr2.70e-04_bs32_wd1.00e-04_dr0.45\n","   27. lr2.70e-04_bs32_wd1.00e-04_dr0.50\n","   28. lr2.70e-04_bs32_wd1.00e-04_dr0.55\n","   29. lr2.70e-04_bs32_wd1.00e-04_dr0.60\n","   30. lr2.70e-04_bs32_wd1.00e-04_dr0.65\n","\n","======================================================================\n","ğŸš€ STARTING HYPERPARAMETER TUNING\n","======================================================================\n","\n","ğŸ“Š Total combinations to test: 30\n","âš™ï¸  Mixed Precision: True\n","âš™ï¸  Gradient Accumulation: 2x\n","\n","======================================================================\n","ğŸ”§ Experiment 1/30: lr1.26e-04_bs32_wd1.00e-04_dr0.45\n","======================================================================\n","   LR: 1e-04, Batch: 32, WD: 1e-04, Dropout: 0.45\n","   Effective Batch: 64\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-4142847985.py:21: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = GradScaler(enabled=use_amp)\n","/tmp/ipython-input-4142847985.py:31: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with autocast(enabled=use_amp):\n","/tmp/ipython-input-4142847985.py:78: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with autocast(enabled=use_amp):\n"]},{"output_type":"stream","name":"stdout","text":["\n","   âœ… Best Val MAE: 62.14Â°\n","   âœ… Acc@15Â°: 30.3%\n","   âœ… Acc@30Â°: 54.3%\n","   âœ… Acc@45Â°: 71.3%\n","   â±ï¸  Time: 0.15 hours\n","\n","ğŸ“Š Results saved to: /content/drive/MyDrive/hyperparameter_tuning_results_round3/hyperparameter_tuning_results.xlsx\n","\n","======================================================================\n","ğŸ”§ Experiment 2/30: lr1.26e-04_bs32_wd1.00e-04_dr0.50\n","======================================================================\n","   LR: 1e-04, Batch: 32, WD: 1e-04, Dropout: 0.5\n","   Effective Batch: 64\n","\n","   âœ… Best Val MAE: 65.08Â°\n","   âœ… Acc@15Â°: 33.3%\n","   âœ… Acc@30Â°: 53.7%\n","   âœ… Acc@45Â°: 69.0%\n","   â±ï¸  Time: 0.15 hours\n","\n","ğŸ“Š Results saved to: /content/drive/MyDrive/hyperparameter_tuning_results_round3/hyperparameter_tuning_results.xlsx\n","\n","======================================================================\n","ğŸ”§ Experiment 3/30: lr1.26e-04_bs32_wd1.00e-04_dr0.55\n","======================================================================\n","   LR: 1e-04, Batch: 32, WD: 1e-04, Dropout: 0.55\n","   Effective Batch: 64\n","\n","   âœ… Best Val MAE: 67.33Â°\n","   âœ… Acc@15Â°: 29.3%\n","   âœ… Acc@30Â°: 53.0%\n","   âœ… Acc@45Â°: 67.7%\n","   â±ï¸  Time: 0.30 hours\n","\n","ğŸ“Š Results saved to: /content/drive/MyDrive/hyperparameter_tuning_results_round3/hyperparameter_tuning_results.xlsx\n","\n","======================================================================\n","ğŸ”§ Experiment 4/30: lr1.26e-04_bs32_wd1.00e-04_dr0.60\n","======================================================================\n","   LR: 1e-04, Batch: 32, WD: 1e-04, Dropout: 0.6\n","   Effective Batch: 64\n","\n","   âœ… Best Val MAE: 65.23Â°\n","   âœ… Acc@15Â°: 34.3%\n","   âœ… Acc@30Â°: 55.7%\n","   âœ… Acc@45Â°: 70.7%\n","   â±ï¸  Time: 0.25 hours\n","\n","ğŸ“Š Results saved to: /content/drive/MyDrive/hyperparameter_tuning_results_round3/hyperparameter_tuning_results.xlsx\n","\n","======================================================================\n","ğŸ”§ Experiment 5/30: lr1.26e-04_bs32_wd1.00e-04_dr0.65\n","======================================================================\n","   LR: 1e-04, Batch: 32, WD: 1e-04, Dropout: 0.65\n","   Effective Batch: 64\n","\n","   âœ… Best Val MAE: 62.01Â°\n","   âœ… Acc@15Â°: 30.7%\n","   âœ… Acc@30Â°: 55.3%\n","   âœ… Acc@45Â°: 71.0%\n","   â±ï¸  Time: 0.27 hours\n","\n","ğŸ“Š Results saved to: /content/drive/MyDrive/hyperparameter_tuning_results_round3/hyperparameter_tuning_results.xlsx\n","\n","======================================================================\n","ğŸ”§ Experiment 6/30: lr1.53e-04_bs32_wd1.00e-04_dr0.45\n","======================================================================\n","   LR: 2e-04, Batch: 32, WD: 1e-04, Dropout: 0.45\n","   Effective Batch: 64\n","\n","   âœ… Best Val MAE: 60.11Â°\n","   âœ… Acc@15Â°: 34.0%\n","   âœ… Acc@30Â°: 57.3%\n","   âœ… Acc@45Â°: 72.7%\n","   â±ï¸  Time: 0.16 hours\n","\n","ğŸ“Š Results saved to: /content/drive/MyDrive/hyperparameter_tuning_results_round3/hyperparameter_tuning_results.xlsx\n","\n","======================================================================\n","ğŸ”§ Experiment 7/30: lr1.53e-04_bs32_wd1.00e-04_dr0.50\n","======================================================================\n","   LR: 2e-04, Batch: 32, WD: 1e-04, Dropout: 0.5\n","   Effective Batch: 64\n","\n","   âœ… Best Val MAE: 58.58Â°\n","   âœ… Acc@15Â°: 35.0%\n","   âœ… Acc@30Â°: 58.7%\n","   âœ… Acc@45Â°: 75.0%\n","   â±ï¸  Time: 0.18 hours\n","\n","ğŸ“Š Results saved to: /content/drive/MyDrive/hyperparameter_tuning_results_round3/hyperparameter_tuning_results.xlsx\n","\n","======================================================================\n","ğŸ”§ Experiment 8/30: lr1.53e-04_bs32_wd1.00e-04_dr0.55\n","======================================================================\n","   LR: 2e-04, Batch: 32, WD: 1e-04, Dropout: 0.55\n","   Effective Batch: 64\n","\n","   âœ… Best Val MAE: 69.58Â°\n","   âœ… Acc@15Â°: 31.7%\n","   âœ… Acc@30Â°: 55.0%\n","   âœ… Acc@45Â°: 69.7%\n","   â±ï¸  Time: 0.18 hours\n","\n","ğŸ“Š Results saved to: /content/drive/MyDrive/hyperparameter_tuning_results_round3/hyperparameter_tuning_results.xlsx\n","\n","======================================================================\n","ğŸ”§ Experiment 9/30: lr1.53e-04_bs32_wd1.00e-04_dr0.60\n","======================================================================\n","   LR: 2e-04, Batch: 32, WD: 1e-04, Dropout: 0.6\n","   Effective Batch: 64\n","\n","   âœ… Best Val MAE: 59.87Â°\n","   âœ… Acc@15Â°: 34.3%\n","   âœ… Acc@30Â°: 55.0%\n","   âœ… Acc@45Â°: 73.0%\n","   â±ï¸  Time: 0.17 hours\n","\n","ğŸ“Š Results saved to: /content/drive/MyDrive/hyperparameter_tuning_results_round3/hyperparameter_tuning_results.xlsx\n","\n","======================================================================\n","ğŸ”§ Experiment 10/30: lr1.53e-04_bs32_wd1.00e-04_dr0.65\n","======================================================================\n","   LR: 2e-04, Batch: 32, WD: 1e-04, Dropout: 0.65\n","   Effective Batch: 64\n","\n","   âœ… Best Val MAE: 63.11Â°\n","   âœ… Acc@15Â°: 36.0%\n","   âœ… Acc@30Â°: 57.0%\n","   âœ… Acc@45Â°: 73.3%\n","   â±ï¸  Time: 0.19 hours\n","\n","ğŸ“Š Results saved to: /content/drive/MyDrive/hyperparameter_tuning_results_round3/hyperparameter_tuning_results.xlsx\n","\n","======================================================================\n","ğŸ”§ Experiment 11/30: lr1.80e-04_bs32_wd1.00e-04_dr0.45\n","======================================================================\n","   LR: 2e-04, Batch: 32, WD: 1e-04, Dropout: 0.45\n","   Effective Batch: 64\n","\n","   âœ… Best Val MAE: 63.52Â°\n","   âœ… Acc@15Â°: 30.7%\n","   âœ… Acc@30Â°: 54.7%\n","   âœ… Acc@45Â°: 72.0%\n","   â±ï¸  Time: 0.23 hours\n","\n","ğŸ“Š Results saved to: /content/drive/MyDrive/hyperparameter_tuning_results_round3/hyperparameter_tuning_results.xlsx\n","\n","======================================================================\n","ğŸ”§ Experiment 12/30: lr1.80e-04_bs32_wd1.00e-04_dr0.50\n","======================================================================\n","   LR: 2e-04, Batch: 32, WD: 1e-04, Dropout: 0.5\n","   Effective Batch: 64\n","\n","   âœ… Best Val MAE: 59.91Â°\n","   âœ… Acc@15Â°: 33.7%\n","   âœ… Acc@30Â°: 57.0%\n","   âœ… Acc@45Â°: 75.3%\n","   â±ï¸  Time: 0.23 hours\n","\n","ğŸ“Š Results saved to: /content/drive/MyDrive/hyperparameter_tuning_results_round3/hyperparameter_tuning_results.xlsx\n","\n","======================================================================\n","ğŸ”§ Experiment 13/30: lr1.80e-04_bs32_wd1.00e-04_dr0.55\n","======================================================================\n","   LR: 2e-04, Batch: 32, WD: 1e-04, Dropout: 0.55\n","   Effective Batch: 64\n","\n","   âœ… Best Val MAE: 58.62Â°\n","   âœ… Acc@15Â°: 32.7%\n","   âœ… Acc@30Â°: 54.3%\n","   âœ… Acc@45Â°: 73.3%\n","   â±ï¸  Time: 0.22 hours\n","\n","ğŸ“Š Results saved to: /content/drive/MyDrive/hyperparameter_tuning_results_round3/hyperparameter_tuning_results.xlsx\n","\n","======================================================================\n","ğŸ”§ Experiment 14/30: lr1.80e-04_bs32_wd1.00e-04_dr0.60\n","======================================================================\n","   LR: 2e-04, Batch: 32, WD: 1e-04, Dropout: 0.6\n","   Effective Batch: 64\n","\n","   âœ… Best Val MAE: 62.72Â°\n","   âœ… Acc@15Â°: 33.7%\n","   âœ… Acc@30Â°: 54.7%\n","   âœ… Acc@45Â°: 73.0%\n","   â±ï¸  Time: 0.22 hours\n","\n","ğŸ“Š Results saved to: /content/drive/MyDrive/hyperparameter_tuning_results_round3/hyperparameter_tuning_results.xlsx\n","\n","======================================================================\n","ğŸ”§ Experiment 15/30: lr1.80e-04_bs32_wd1.00e-04_dr0.65\n","======================================================================\n","   LR: 2e-04, Batch: 32, WD: 1e-04, Dropout: 0.65\n","   Effective Batch: 64\n","\n","   âœ… Best Val MAE: 69.57Â°\n","   âœ… Acc@15Â°: 29.7%\n","   âœ… Acc@30Â°: 51.3%\n","   âœ… Acc@45Â°: 67.3%\n","   â±ï¸  Time: 0.19 hours\n","\n","ğŸ“Š Results saved to: /content/drive/MyDrive/hyperparameter_tuning_results_round3/hyperparameter_tuning_results.xlsx\n","\n","======================================================================\n","ğŸ”§ Experiment 16/30: lr2.07e-04_bs32_wd1.00e-04_dr0.45\n","======================================================================\n","   LR: 2e-04, Batch: 32, WD: 1e-04, Dropout: 0.45\n","   Effective Batch: 64\n","\n","   âœ… Best Val MAE: 55.34Â°\n","   âœ… Acc@15Â°: 33.3%\n","   âœ… Acc@30Â°: 55.0%\n","   âœ… Acc@45Â°: 72.7%\n","   â±ï¸  Time: 0.20 hours\n","\n","ğŸ“Š Results saved to: /content/drive/MyDrive/hyperparameter_tuning_results_round3/hyperparameter_tuning_results.xlsx\n","\n","======================================================================\n","ğŸ”§ Experiment 17/30: lr2.07e-04_bs32_wd1.00e-04_dr0.50\n","======================================================================\n","   LR: 2e-04, Batch: 32, WD: 1e-04, Dropout: 0.5\n","   Effective Batch: 64\n","\n","   âœ… Best Val MAE: 63.68Â°\n","   âœ… Acc@15Â°: 36.3%\n","   âœ… Acc@30Â°: 58.0%\n","   âœ… Acc@45Â°: 71.0%\n","   â±ï¸  Time: 0.22 hours\n","\n","ğŸ“Š Results saved to: /content/drive/MyDrive/hyperparameter_tuning_results_round3/hyperparameter_tuning_results.xlsx\n","\n","======================================================================\n","ğŸ”§ Experiment 18/30: lr2.07e-04_bs32_wd1.00e-04_dr0.55\n","======================================================================\n","   LR: 2e-04, Batch: 32, WD: 1e-04, Dropout: 0.55\n","   Effective Batch: 64\n","\n","   âœ… Best Val MAE: 62.56Â°\n","   âœ… Acc@15Â°: 34.3%\n","   âœ… Acc@30Â°: 53.7%\n","   âœ… Acc@45Â°: 71.7%\n","   â±ï¸  Time: 0.23 hours\n","\n","ğŸ“Š Results saved to: /content/drive/MyDrive/hyperparameter_tuning_results_round3/hyperparameter_tuning_results.xlsx\n","\n","======================================================================\n","ğŸ”§ Experiment 19/30: lr2.07e-04_bs32_wd1.00e-04_dr0.60\n","======================================================================\n","   LR: 2e-04, Batch: 32, WD: 1e-04, Dropout: 0.6\n","   Effective Batch: 64\n","\n","   âœ… Best Val MAE: 65.78Â°\n","   âœ… Acc@15Â°: 33.3%\n","   âœ… Acc@30Â°: 56.0%\n","   âœ… Acc@45Â°: 69.3%\n","   â±ï¸  Time: 0.16 hours\n","\n","ğŸ“Š Results saved to: /content/drive/MyDrive/hyperparameter_tuning_results_round3/hyperparameter_tuning_results.xlsx\n","\n","======================================================================\n","ğŸ”§ Experiment 20/30: lr2.07e-04_bs32_wd1.00e-04_dr0.65\n","======================================================================\n","   LR: 2e-04, Batch: 32, WD: 1e-04, Dropout: 0.65\n","   Effective Batch: 64\n","\n","   âœ… Best Val MAE: 62.31Â°\n","   âœ… Acc@15Â°: 33.3%\n","   âœ… Acc@30Â°: 57.3%\n","   âœ… Acc@45Â°: 72.3%\n","   â±ï¸  Time: 0.25 hours\n","\n","ğŸ“Š Results saved to: /content/drive/MyDrive/hyperparameter_tuning_results_round3/hyperparameter_tuning_results.xlsx\n","\n","======================================================================\n","ğŸ”§ Experiment 21/30: lr2.34e-04_bs32_wd1.00e-04_dr0.45\n","======================================================================\n","   LR: 2e-04, Batch: 32, WD: 1e-04, Dropout: 0.45\n","   Effective Batch: 64\n","\n","   âœ… Best Val MAE: 69.09Â°\n","   âœ… Acc@15Â°: 33.7%\n","   âœ… Acc@30Â°: 54.0%\n","   âœ… Acc@45Â°: 69.7%\n","   â±ï¸  Time: 0.22 hours\n","\n","ğŸ“Š Results saved to: /content/drive/MyDrive/hyperparameter_tuning_results_round3/hyperparameter_tuning_results.xlsx\n","\n","======================================================================\n","ğŸ”§ Experiment 22/30: lr2.34e-04_bs32_wd1.00e-04_dr0.50\n","======================================================================\n","   LR: 2e-04, Batch: 32, WD: 1e-04, Dropout: 0.5\n","   Effective Batch: 64\n","\n","   âœ… Best Val MAE: 62.33Â°\n","   âœ… Acc@15Â°: 35.3%\n","   âœ… Acc@30Â°: 55.3%\n","   âœ… Acc@45Â°: 70.7%\n","   â±ï¸  Time: 0.19 hours\n","\n","ğŸ“Š Results saved to: /content/drive/MyDrive/hyperparameter_tuning_results_round3/hyperparameter_tuning_results.xlsx\n","\n","======================================================================\n","ğŸ”§ Experiment 23/30: lr2.34e-04_bs32_wd1.00e-04_dr0.55\n","======================================================================\n","   LR: 2e-04, Batch: 32, WD: 1e-04, Dropout: 0.55\n","   Effective Batch: 64\n","\n","   âœ… Best Val MAE: 64.71Â°\n","   âœ… Acc@15Â°: 31.0%\n","   âœ… Acc@30Â°: 55.7%\n","   âœ… Acc@45Â°: 72.0%\n","   â±ï¸  Time: 0.16 hours\n","\n","ğŸ“Š Results saved to: /content/drive/MyDrive/hyperparameter_tuning_results_round3/hyperparameter_tuning_results.xlsx\n","\n","======================================================================\n","ğŸ”§ Experiment 24/30: lr2.34e-04_bs32_wd1.00e-04_dr0.60\n","======================================================================\n","   LR: 2e-04, Batch: 32, WD: 1e-04, Dropout: 0.6\n","   Effective Batch: 64\n","\n","   âœ… Best Val MAE: 62.99Â°\n","   âœ… Acc@15Â°: 34.0%\n","   âœ… Acc@30Â°: 58.0%\n","   âœ… Acc@45Â°: 70.7%\n","   â±ï¸  Time: 0.24 hours\n","\n","ğŸ“Š Results saved to: /content/drive/MyDrive/hyperparameter_tuning_results_round3/hyperparameter_tuning_results.xlsx\n","\n","======================================================================\n","ğŸ”§ Experiment 25/30: lr2.34e-04_bs32_wd1.00e-04_dr0.65\n","======================================================================\n","   LR: 2e-04, Batch: 32, WD: 1e-04, Dropout: 0.65\n","   Effective Batch: 64\n","\n","   âœ… Best Val MAE: 60.90Â°\n","   âœ… Acc@15Â°: 33.3%\n","   âœ… Acc@30Â°: 54.7%\n","   âœ… Acc@45Â°: 71.0%\n","   â±ï¸  Time: 0.20 hours\n","\n","ğŸ“Š Results saved to: /content/drive/MyDrive/hyperparameter_tuning_results_round3/hyperparameter_tuning_results.xlsx\n","\n","======================================================================\n","ğŸ”§ Experiment 26/30: lr2.70e-04_bs32_wd1.00e-04_dr0.45\n","======================================================================\n","   LR: 3e-04, Batch: 32, WD: 1e-04, Dropout: 0.45\n","   Effective Batch: 64\n","\n","   âœ… Best Val MAE: 59.64Â°\n","   âœ… Acc@15Â°: 32.3%\n","   âœ… Acc@30Â°: 54.0%\n","   âœ… Acc@45Â°: 72.0%\n","   â±ï¸  Time: 0.16 hours\n","\n","ğŸ“Š Results saved to: /content/drive/MyDrive/hyperparameter_tuning_results_round3/hyperparameter_tuning_results.xlsx\n","\n","======================================================================\n","ğŸ”§ Experiment 27/30: lr2.70e-04_bs32_wd1.00e-04_dr0.50\n","======================================================================\n","   LR: 3e-04, Batch: 32, WD: 1e-04, Dropout: 0.5\n","   Effective Batch: 64\n","\n","   âœ… Best Val MAE: 59.23Â°\n","   âœ… Acc@15Â°: 34.0%\n","   âœ… Acc@30Â°: 55.7%\n","   âœ… Acc@45Â°: 72.7%\n","   â±ï¸  Time: 0.31 hours\n","\n","ğŸ“Š Results saved to: /content/drive/MyDrive/hyperparameter_tuning_results_round3/hyperparameter_tuning_results.xlsx\n","\n","======================================================================\n","ğŸ”§ Experiment 28/30: lr2.70e-04_bs32_wd1.00e-04_dr0.55\n","======================================================================\n","   LR: 3e-04, Batch: 32, WD: 1e-04, Dropout: 0.55\n","   Effective Batch: 64\n","\n","   âœ… Best Val MAE: 58.68Â°\n","   âœ… Acc@15Â°: 35.7%\n","   âœ… Acc@30Â°: 55.7%\n","   âœ… Acc@45Â°: 73.0%\n","   â±ï¸  Time: 0.18 hours\n","\n","ğŸ“Š Results saved to: /content/drive/MyDrive/hyperparameter_tuning_results_round3/hyperparameter_tuning_results.xlsx\n","\n","======================================================================\n","ğŸ”§ Experiment 29/30: lr2.70e-04_bs32_wd1.00e-04_dr0.60\n","======================================================================\n","   LR: 3e-04, Batch: 32, WD: 1e-04, Dropout: 0.6\n","   Effective Batch: 64\n","\n","   âœ… Best Val MAE: 59.02Â°\n","   âœ… Acc@15Â°: 34.3%\n","   âœ… Acc@30Â°: 54.7%\n","   âœ… Acc@45Â°: 71.7%\n","   â±ï¸  Time: 0.22 hours\n","\n","ğŸ“Š Results saved to: /content/drive/MyDrive/hyperparameter_tuning_results_round3/hyperparameter_tuning_results.xlsx\n","\n","======================================================================\n","ğŸ”§ Experiment 30/30: lr2.70e-04_bs32_wd1.00e-04_dr0.65\n","======================================================================\n","   LR: 3e-04, Batch: 32, WD: 1e-04, Dropout: 0.65\n","   Effective Batch: 64\n","\n","   âœ… Best Val MAE: 70.19Â°\n","   âœ… Acc@15Â°: 36.7%\n","   âœ… Acc@30Â°: 57.3%\n","   âœ… Acc@45Â°: 72.0%\n","   â±ï¸  Time: 0.25 hours\n","\n","ğŸ“Š Results saved to: /content/drive/MyDrive/hyperparameter_tuning_results_round3/hyperparameter_tuning_results.xlsx\n","\n","======================================================================\n","ğŸ‰ HYPERPARAMETER TUNING COMPLETED!\n","======================================================================\n","âœ… Completed: 30\n","â­ï¸  Skipped: 0\n","ğŸ“Š Total results: 30\n","======================================================================\n","\n","ğŸ“Š Results saved to: /content/drive/MyDrive/hyperparameter_tuning_results_round3/hyperparameter_tuning_results.xlsx\n","\n","ğŸ† TOP 10 RESULTS:\n","======================================================================\n","1. lr2.07e-04_bs32_wd1.00e-04_dr0.45\n","   MAE: 55.34Â° | Acc@15Â°: 33.3% | Acc@30Â°: 55.0%\n","2. lr1.53e-04_bs32_wd1.00e-04_dr0.50\n","   MAE: 58.58Â° | Acc@15Â°: 35.0% | Acc@30Â°: 58.7%\n","3. lr1.80e-04_bs32_wd1.00e-04_dr0.55\n","   MAE: 58.62Â° | Acc@15Â°: 32.7% | Acc@30Â°: 54.3%\n","4. lr2.70e-04_bs32_wd1.00e-04_dr0.55\n","   MAE: 58.68Â° | Acc@15Â°: 35.7% | Acc@30Â°: 55.7%\n","5. lr2.70e-04_bs32_wd1.00e-04_dr0.60\n","   MAE: 59.02Â° | Acc@15Â°: 34.3% | Acc@30Â°: 54.7%\n","6. lr2.70e-04_bs32_wd1.00e-04_dr0.50\n","   MAE: 59.23Â° | Acc@15Â°: 34.0% | Acc@30Â°: 55.7%\n","7. lr2.70e-04_bs32_wd1.00e-04_dr0.45\n","   MAE: 59.64Â° | Acc@15Â°: 32.3% | Acc@30Â°: 54.0%\n","8. lr1.53e-04_bs32_wd1.00e-04_dr0.60\n","   MAE: 59.87Â° | Acc@15Â°: 34.3% | Acc@30Â°: 55.0%\n","9. lr1.80e-04_bs32_wd1.00e-04_dr0.50\n","   MAE: 59.91Â° | Acc@15Â°: 33.7% | Acc@30Â°: 57.0%\n","10. lr1.53e-04_bs32_wd1.00e-04_dr0.45\n","   MAE: 60.11Â° | Acc@15Â°: 34.0% | Acc@30Â°: 57.3%\n","======================================================================\n"]}],"source":["# DEBUG: Check for duplicates\n","print(\"ğŸ” Checking for duplicate experiment names...\")\n","from collections import Counter\n","\n","param_combinations_test = list(itertools.product(\n","    HYPERPARAMETER_GRID['learning_rate'],\n","    HYPERPARAMETER_GRID['batch_size'],\n","    HYPERPARAMETER_GRID['weight_decay'],\n","    HYPERPARAMETER_GRID['dropout']\n","))\n","\n","exp_names = []\n","for lr, bs, wd, dropout in param_combinations_test:\n","    params = {'learning_rate': lr, 'batch_size': bs, 'weight_decay': wd, 'dropout': dropout}\n","    exp_name = get_experiment_name(params)\n","    exp_names.append(exp_name)\n","\n","# Check for duplicates\n","name_counts = Counter(exp_names)\n","duplicates = {name: count for name, count in name_counts.items() if count > 1}\n","\n","if duplicates:\n","    print(f\"âŒ Found {len(duplicates)} duplicate names:\")\n","    for name, count in list(duplicates.items())[:5]:\n","        print(f\"   {name}: appears {count} times\")\n","else:\n","    print(f\"âœ… All {len(exp_names)} experiment names are unique!\")\n","\n","print(f\"\\nğŸ“Š First 5 experiment names:\")\n","for i, name in enumerate(exp_names[:5]):\n","    print(f\"   {i+1}. {name}\")\n","\n","print(f\"\\nğŸ“Š Last 5 experiment names:\")\n","for i, name in enumerate(exp_names[-5:], len(exp_names)-4):\n","    print(f\"   {i}. {name}\")\n","print(\"\\n\" + \"=\"*70)\n","print(\"ğŸš€ STARTING HYPERPARAMETER TUNING\")\n","print(\"=\"*70)\n","\n","# Create results directory\n","results_dir = Path(BASE_CONFIG['results_dir'])\n","results_dir.mkdir(parents=True, exist_ok=True)\n","\n","# Generate all combinations\n","param_combinations = list(itertools.product(\n","    HYPERPARAMETER_GRID['learning_rate'],\n","    HYPERPARAMETER_GRID['batch_size'],\n","    HYPERPARAMETER_GRID['weight_decay'],\n","    HYPERPARAMETER_GRID['dropout']\n","))\n","\n","print(f\"\\nğŸ“Š Total combinations to test: {len(param_combinations)}\")\n","print(f\"âš™ï¸  Mixed Precision: {FIXED_PARAMS['use_amp']}\")\n","print(f\"âš™ï¸  Gradient Accumulation: {FIXED_PARAMS['gradient_accumulation_steps']}x\")\n","\n","# Track all results\n","all_results = []\n","completed_count = 0\n","skipped_count = 0\n","\n","for idx, (lr, bs, wd, dropout) in enumerate(param_combinations, 1):\n","    params = {\n","        'learning_rate': lr,\n","        'batch_size': bs,\n","        'weight_decay': wd,\n","        'dropout': dropout\n","    }\n","\n","    exp_name = get_experiment_name(params)\n","\n","    print(f\"\\n{'='*70}\")\n","    print(f\"ğŸ”§ Experiment {idx}/{len(param_combinations)}: {exp_name}\")\n","    print(f\"{'='*70}\")\n","    print(f\"   LR: {lr:.0e}, Batch: {bs}, WD: {wd:.0e}, Dropout: {dropout}\")\n","    print(f\"   Effective Batch: {bs * FIXED_PARAMS['gradient_accumulation_steps']}\")\n","\n","    # Check if already completed\n","    if check_if_completed(results_dir, exp_name):\n","        print(f\"   â­ï¸  Already completed, skipping...\")\n","        skipped_count += 1\n","\n","        # Load existing result\n","        with open(results_dir / exp_name / 'result.json', 'r') as f:\n","            result = json.load(f)\n","        all_results.append(result)\n","        continue\n","\n","    try:\n","        # Create data loaders with current batch size\n","        train_loader = DataLoader(\n","            train_dataset,\n","            batch_size=bs,\n","            shuffle=True,\n","            num_workers=BASE_CONFIG['num_workers'],\n","            pin_memory=BASE_CONFIG['pin_memory']\n","        )\n","\n","        val_loader = DataLoader(\n","            val_dataset,\n","            batch_size=bs,\n","            shuffle=False,\n","            num_workers=BASE_CONFIG['num_workers'],\n","            pin_memory=BASE_CONFIG['pin_memory']\n","        )\n","\n","        # Train\n","        result = train_single_experiment(\n","            params, train_loader, val_loader, device, results_dir\n","        )\n","\n","        all_results.append(result)\n","        completed_count += 1\n","\n","        print(f\"\\n   âœ… Best Val MAE: {result['best_val_mae']:.2f}Â°\")\n","        print(f\"   âœ… Acc@15Â°: {result['best_val_acc15']:.1f}%\")\n","        print(f\"   âœ… Acc@30Â°: {result['best_val_acc30']:.1f}%\")\n","        print(f\"   âœ… Acc@45Â°: {result['best_val_acc45']:.1f}%\")\n","        print(f\"   â±ï¸  Time: {result['training_time_hours']:.2f} hours\")\n","\n","        # Save results to Excel after each experiment\n","        save_results_to_excel(results_dir, all_results)\n","\n","        # Clear GPU cache\n","        torch.cuda.empty_cache()\n","\n","    except Exception as e:\n","        print(f\"\\n   âŒ Error in experiment {exp_name}: {str(e)}\")\n","        continue\n","\n","print(\"\\n\" + \"=\"*70)\n","print(\"ğŸ‰ HYPERPARAMETER TUNING COMPLETED!\")\n","print(\"=\"*70)\n","print(f\"âœ… Completed: {completed_count}\")\n","print(f\"â­ï¸  Skipped: {skipped_count}\")\n","print(f\"ğŸ“Š Total results: {len(all_results)}\")\n","print(\"=\"*70)\n","\n","# Final save\n","if all_results:\n","    excel_file = save_results_to_excel(results_dir, all_results)\n","\n","    # Show top 10 results\n","    sorted_results = sorted(all_results, key=lambda x: x['best_val_mae'])\n","    print(\"\\nğŸ† TOP 10 RESULTS:\")\n","    print(\"=\"*70)\n","    for i, res in enumerate(sorted_results[:10], 1):\n","        print(f\"{i}. {res['experiment_name']}\")\n","        print(f\"   MAE: {res['best_val_mae']:.2f}Â° | Acc@15Â°: {res['best_val_acc15']:.1f}% | Acc@30Â°: {res['best_val_acc30']:.1f}%\")\n","    print(\"=\"*70)"]},{"cell_type":"markdown","metadata":{"id":"e0FplEL-7O6f"},"source":["## 11. Analyze Results"]},{"cell_type":"code","execution_count":45,"metadata":{"id":"nDMD7OmV7O6f","executionInfo":{"status":"ok","timestamp":1766553997578,"user_tz":-180,"elapsed":77,"user":{"displayName":"ofcosar","userId":"12337476720676369762"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"c466a5c5-792b-4b1c-b970-c5308093d0ed"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","ğŸ“Š HYPERPARAMETER ANALYSIS\n","======================================================================\n","\n","ğŸ¥‡ BEST CONFIGURATION:\n","   Experiment: lr2.07e-04_bs32_wd1.00e-04_dr0.45\n","   LR: 2e-04, Batch: 32, WD: 1e-04, Dropout: 0.45\n","   Val MAE: 55.34Â°\n","   Acc@15Â°: 33.3%\n","   Acc@30Â°: 55.0%\n","   Acc@45Â°: 72.7%\n","   Training time: 0.20 hours\n","\n","ğŸ“ˆ OVERALL STATISTICS:\n","   Mean Val MAE: 62.79Â°\n","   Std Val MAE: 3.74Â°\n","   Min Val MAE: 55.34Â°\n","   Max Val MAE: 70.19Â°\n","\n","   Mean Acc@30Â°: 55.4%\n","   Max Acc@30Â°: 58.7%\n","\n","ğŸ“Š BEST BY LEARNING RATE:\n","   LR 1e-04: MAE 62.01Â° | Acc@30Â° 55.3%\n","   LR 2e-04: MAE 58.58Â° | Acc@30Â° 58.7%\n","   LR 2e-04: MAE 58.62Â° | Acc@30Â° 54.3%\n","   LR 2e-04: MAE 55.34Â° | Acc@30Â° 55.0%\n","   LR 2e-04: MAE 60.90Â° | Acc@30Â° 54.7%\n","   LR 3e-04: MAE 58.68Â° | Acc@30Â° 55.7%\n","\n","ğŸ“Š BEST BY BATCH SIZE:\n","   Batch  32: MAE 55.34Â° | Acc@30Â° 55.0%\n","\n","======================================================================\n","ğŸ“ Full results saved to: /content/drive/MyDrive/hyperparameter_tuning_results_round3/hyperparameter_tuning_results.xlsx\n","======================================================================\n"]}],"source":["# Load results from Excel\n","excel_file = Path(BASE_CONFIG['results_dir']) / 'hyperparameter_tuning_results.xlsx'\n","\n","if excel_file.exists():\n","    df = pd.read_excel(excel_file)\n","\n","    print(\"\\nğŸ“Š HYPERPARAMETER ANALYSIS\")\n","    print(\"=\"*70)\n","\n","    # Best overall\n","    best = df.iloc[0]\n","    print(f\"\\nğŸ¥‡ BEST CONFIGURATION:\")\n","    print(f\"   Experiment: {best['experiment_name']}\")\n","    print(f\"   LR: {best['learning_rate']:.0e}, Batch: {best['batch_size']}, WD: {best['weight_decay']:.0e}, Dropout: {best['dropout']}\")\n","    print(f\"   Val MAE: {best['best_val_mae']:.2f}Â°\")\n","    print(f\"   Acc@15Â°: {best['best_val_acc15']:.1f}%\")\n","    print(f\"   Acc@30Â°: {best['best_val_acc30']:.1f}%\")\n","    print(f\"   Acc@45Â°: {best['best_val_acc45']:.1f}%\")\n","    print(f\"   Training time: {best['training_time_hours']:.2f} hours\")\n","\n","    # Statistics\n","    print(f\"\\nğŸ“ˆ OVERALL STATISTICS:\")\n","    print(f\"   Mean Val MAE: {df['best_val_mae'].mean():.2f}Â°\")\n","    print(f\"   Std Val MAE: {df['best_val_mae'].std():.2f}Â°\")\n","    print(f\"   Min Val MAE: {df['best_val_mae'].min():.2f}Â°\")\n","    print(f\"   Max Val MAE: {df['best_val_mae'].max():.2f}Â°\")\n","    print(f\"\\n   Mean Acc@30Â°: {df['best_val_acc30'].mean():.1f}%\")\n","    print(f\"   Max Acc@30Â°: {df['best_val_acc30'].max():.1f}%\")\n","\n","    # Best by learning rate\n","    print(f\"\\nğŸ“Š BEST BY LEARNING RATE:\")\n","    for lr in sorted(df['learning_rate'].unique()):\n","        lr_df = df[df['learning_rate'] == lr]\n","        best_lr = lr_df.iloc[0]\n","        print(f\"   LR {lr:.0e}: MAE {best_lr['best_val_mae']:.2f}Â° | Acc@30Â° {best_lr['best_val_acc30']:.1f}%\")\n","\n","    # Best by batch size\n","    print(f\"\\nğŸ“Š BEST BY BATCH SIZE:\")\n","    for bs in sorted(df['batch_size'].unique()):\n","        bs_df = df[df['batch_size'] == bs]\n","        best_bs = bs_df.iloc[0]\n","        print(f\"   Batch {bs:3d}: MAE {best_bs['best_val_mae']:.2f}Â° | Acc@30Â° {best_bs['best_val_acc30']:.1f}%\")\n","\n","    print(\"\\n\" + \"=\"*70)\n","    print(f\"ğŸ“ Full results saved to: {excel_file}\")\n","    print(\"=\"*70)\n","else:\n","    print(\"âš ï¸ No results file found. Run the tuning experiment first!\")"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}